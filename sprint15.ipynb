{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件：<br>\n",
    "答える際は論文のどの部分からそれが分かるかを書く。<br>\n",
    "必要に応じて先行研究（引用されている論文）も探しにいく。最低2つは他の論文を利用して回答すること。<br>\n",
    "論文の紹介記事を見ても良い。ただし、答えは論文内に根拠を探すこと。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：Faster R-CNNの全身となるFast R-CNNが存在した。①<br>\n",
    "Fast R-CNNは、下記ステップを踏む。②<br>\n",
    "1. 全体の画像から特徴マップを生成する\n",
    "2. それぞれの物体提案に対してROIプーリングを行い固定長の特徴ベクトルを特徴マップから抽出する\n",
    "3. クラス予測を行うbranchとbbox予測を行うbranchに別れる前の全結合レイヤにそれぞれの特徴ベクトルが入力される\n",
    "4. 最終的に、K個のクラスに対する予測と４つの値にエンコードされたbboxのポジションが出力される\n",
    "\n",
    "<br>\n",
    "①<br>\n",
    "引用元：1.INTRODUCTIONの7行目<br>\n",
    "引用文：The latest incarnation, Fast R-CNN [2], achieves near real-time rates using very deep networks [3], when ignoring the time spent on region proposals. \n",
    "\n",
    "②<br>\n",
    "引用元：別論文 [Fast R-CNN]の 2. Fast R-CNN architecture and trainingの１段落目全文<br>\n",
    "引用文：<br>\n",
    "Fig. 1 illustrates the Fast R-CNN architecture. A Fast\n",
    "R-CNN network takes as input an entire image and a set\n",
    "of object proposals. The network first processes the whole\n",
    "image with several convolutional (conv) and max pooling\n",
    "layers to produce a conv feature map. Then, for each object proposal a region of interest (RoI) pooling layer extracts a fixed-length feature vector from the feature map.\n",
    "Each feature vector is fed into a sequence of fully connected\n",
    "(fc) layers that finally branch into two sibling output layers: one that produces softmax probability estimates over\n",
    "K object classes plus a catch-all “background” class and\n",
    "another layer that outputs four real-valued numbers for each\n",
    "of the K object classes. Each set of 4 values encodes refined\n",
    "bounding-box positions for one of the K classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：最新の物体検出と畳み込み層を共有するRPNsを導入し、オブジェクト提案のコストを削減することで高速化した<br>\n",
    "引用元：INTRODUCTION 4段落目<br>\n",
    "引用文：In this paper, we show that an algorithmic change—\n",
    "computing proposals with a deep convolutional neural network—leads to an elegant and effective solution\n",
    "where proposal computation is nearly cost-free given\n",
    "the detection network’s computation. To this end, we\n",
    "introduce novel Region Proposal Networks (RPNs) that\n",
    "share convolutional layers with state-of-the-art object\n",
    "detection networks [1], [2]. By sharing convolutions at\n",
    "test-time, the marginal cost for computing proposals\n",
    "is small (e.g., 10ms per image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：One-Stageは、領域提案というステップを挟まないが、Two-Stageは領域提案を行なった上で、物体検出を行う<br>\n",
    "引用元：4 EXPERIMENTS One-Stage Detection vs. Two-Stage Proposal + Detection.<br>\n",
    "引用文：The OverFeat paper [9] proposes a detection\n",
    "method that uses regressors and classifiers on sliding\n",
    "windows over convolutional feature maps. OverFeat\n",
    "is a one-stage, class-specific detection pipeline, and ours\n",
    "is a two-stage cascade consisting of class-agnostic proposals and class-specific detections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：RPNは特徴マップ（image）を入力しそれぞれオブジェクトらしさのスコアを保持した長方形の領域提案のセットを出力する。①<br>\n",
    "領域提案を生成するために、小さな畳み込みネットワークを共有された畳み込み層によって出力された畳み込み特徴マップにスライドさせる②<br>\n",
    "\n",
    "①<br>\n",
    "引用元：3.1 Region Proposal Networks 一段落<br>\n",
    "引用文：A Region Proposal Network (RPN) takes an image (of any size) as input and outputs a set of rectangular object proposals, each with an objectness score.<br>\n",
    "\n",
    "②<br>\n",
    "引用元：3.1 Region Proposal Networks 二段落<br>\n",
    "引用文：To generate region proposals, we slide a small network over the convolutional feature map output by the last shared convolutional layer.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) RoIプーリングとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：ROIプーリングはマックスプーリングを使用して、関心のある領域を小さな特徴マップに変換する。このとき、ハイパーパラメータとして縦横の大きさを設定し、固定長の特徴マップに変換する。RoIは、入力として受け取る一つの畳み込み特徴マップに対する一つの長方形のウィンドウであり、それぞれのRoIは、トップレフトコーナー(r,c)とそのウィンドウの高さと幅(h, w)によって定義される。<br>\n",
    "引用元：別論文 [Fast R-CNN]の 2.1. The RoI pooling layerの１段落目全文\n",
    "引用文：The RoI pooling layer uses max pooling to convert the\n",
    "features inside any valid region of interest into a small feature map with a fixed spatial extent of H × W (e.g., 7 × 7),\n",
    "where H and W are layer hyper-parameters that are independent of any particular RoI. In this paper, an RoI is a\n",
    "rectangular window into a conv feature map. Each RoI is\n",
    "defined by a four-tuple (r, c, h, w) that specifies its top-left\n",
    "corner (r, c) and its height and width (h, w)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Anchorのサイズはどうするのが適切か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：3つのスケールと、３つのアスペクト比を組み合わせた合計９種類のサイズを使うのがデフォルトである。<br>\n",
    "引用元：3.1.1 Anchors<br>\n",
    "引用文：<br>\n",
    "By default we use 3 scales and 3 aspect ratios, yielding k = 9 anchors at each sliding position. For a convolutional feature map of a size W × H (typically ∼2,400), there are W Hk anchors in total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解答：COCOデータセットを使い、先行研究であるFast R-CNNと比較している。Fast R-CNNに置いては、テスト開発セットに置いての39.3% mAP@0.5の精度を出した一方で、Faster R-CNNに置いては42.1% mAP@0.5 and 21.5% mAP@[.5, .95]の精度を出した。これは、mAP@0.5においては2.8%、mAP@[.5, .95]に置いては2.2%高い結果である。<br>\n",
    "引用元：4.2 Experiments on MS COCO 4,5段落目<br>\n",
    "引用文：In Table 11 we first report the results of the Fast\n",
    "R-CNN system [2] using the implementation in this\n",
    "paper. Our Fast R-CNN baseline has 39.3% mAP@0.5\n",
    "on the test-dev set, higher than that reported in [2].\n",
    "We conjecture that the reason for this gap is mainly\n",
    "due to the definition of the negative samples and also\n",
    "the changes of the mini-batch sizes. We also note that\n",
    "the mAP@[.5, .95] is just comparable.\n",
    "Next we evaluate our Faster R-CNN system. Using\n",
    "the COCO training set to train, Faster R-CNN has\n",
    "42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the\n",
    "COCO test-dev set. This is 2.8% higher for mAP@0.5\n",
    "and 2.2% higher for mAP@[.5, .95] than the Fast RCNN counterpart under the same protocol (Table 11).\n",
    "This indicates that RPN performs excellent for improving the localization accuracy at higher IoU thresholds. Using the COCO trainval set to train, Faster RCNN has 42.7% mAP@0.5 and 21.9% mAP@[.5, .95] on\n",
    "the COCO test-dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
