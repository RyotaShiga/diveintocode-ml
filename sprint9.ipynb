{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題1 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n",
      "(28, 28)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) # (60000, 28, 28)\n",
    "print(X_test.shape) # (10000, 28, 28)\n",
    "print(X_train[0].dtype) # uint8\n",
    "print(X_train[0].shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max())\n",
    "print(X_train.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'label : 5')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.33823529 -105.27941176 -105.27941176\n",
      "  -105.27941176 -104.85588235 -104.81666667 -104.66372549 -105.24803922\n",
      "  -104.69901961 -104.35       -104.38137255 -104.85196078 -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.23235294 -105.20882353\n",
      "  -104.98137255 -104.74607843 -104.68333333 -104.35784314 -104.35784314\n",
      "  -104.35784314 -104.35784314 -104.35784314 -104.46764706 -104.6754902\n",
      "  -104.35784314 -104.40098039 -104.58529412 -105.09901961 -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.15784314 -104.41666667 -104.35784314\n",
      "  -104.35784314 -104.35784314 -104.35784314 -104.35784314 -104.35784314\n",
      "  -104.35784314 -104.35784314 -104.36568627 -104.98529412 -105.02843137\n",
      "  -105.02843137 -105.13039216 -105.19705882 -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.27941176 -104.49117647 -104.35784314\n",
      "  -104.35784314 -104.35784314 -104.35784314 -104.35784314 -104.57352941\n",
      "  -104.63627451 -104.38137255 -104.40490196 -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.03627451 -104.73823529\n",
      "  -104.93039216 -104.35784314 -104.35784314 -104.54607843 -105.30686275\n",
      "  -105.35       -105.18137255 -104.74607843 -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.29509804\n",
      "  -105.34607843 -104.74607843 -104.35784314 -104.99705882 -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -104.80490196 -104.35784314 -104.60490196 -105.34215686\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.30686275 -104.60490196 -104.35784314 -105.0754902\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.2127451  -104.40490196 -104.46764706\n",
      "  -104.72254902 -104.92647059 -105.34607843 -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.03235294 -104.40882353\n",
      "  -104.35784314 -104.35784314 -104.88333333 -105.25196078 -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.17352941\n",
      "  -104.62058824 -104.35784314 -104.35784314 -104.76176471 -105.24411765\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.2872549  -104.98529412 -104.36176471 -104.35784314 -104.61666667\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -104.37352941 -104.35784314 -104.37352941\n",
      "  -105.09901961 -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.16960784\n",
      "  -104.84019608 -104.63235294 -104.35784314 -104.35784314 -104.53823529\n",
      "  -105.34215686 -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.19705882 -104.76960784 -104.45196078\n",
      "  -104.35784314 -104.35784314 -104.35784314 -104.36960784 -104.63627451\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.25588235 -104.90294118 -104.48333333 -104.35784314 -104.35784314\n",
      "  -104.35784314 -104.35784314 -104.56176471 -105.04411765 -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.25980392 -105.09117647\n",
      "  -104.51470588 -104.35784314 -104.35784314 -104.35784314 -104.35784314\n",
      "  -104.57352941 -105.03235294 -105.34215686 -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.27941176 -104.67941176 -104.49117647 -104.35784314\n",
      "  -104.35784314 -104.35784314 -104.35784314 -104.58529412 -105.03627451\n",
      "  -105.31470588 -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.13431373\n",
      "  -104.6754902  -104.46372549 -104.35784314 -104.35784314 -104.35784314\n",
      "  -104.35784314 -104.39313725 -104.82843137 -105.30686275 -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -104.81666667\n",
      "  -104.35784314 -104.35784314 -104.35784314 -104.51862745 -104.82058824\n",
      "  -104.83235294 -105.2872549  -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]\n",
      " [-105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35       -105.35       -105.35\n",
      "  -105.35       -105.35       -105.35      ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "index = 0\n",
    "image = X_train[index].reshape(28,28)\n",
    "image = image.astype(np.float) # float型に変換\n",
    "image -= 105.35 # 意図的に負の小数値を作り出してみる\n",
    "plt.imshow(image, 'gray')\n",
    "plt.title('label : {}'.format(y_train[index]))\n",
    "plt.show()\n",
    "print(image) # 値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int)\n",
    "        #self.stopは作成するバッチサイズ数。１エポック分作成する\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    #指定したバッチ番号を取ってきてくれる\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0 : p1], self._y[p0 : p1]\n",
    "\n",
    "    \n",
    "    \n",
    "    #batchカウンターを初期化する\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    #batchを前から一つずつ取ってくる\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter * self.batch_size\n",
    "        p1 = self._counter * self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0 : p1], self._y[p0 : p1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n",
      "(60000, 10)\n",
      "float32\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# enc = OneHotEncoder()\n",
    "# y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=10)\n",
    "print(y_train.shape)\n",
    "print(y_train_one_hot.shape) # (60000, 10)\n",
    "print(y_train_one_hot.dtype) # float64\n",
    "print(type(y_train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchSimpleNeuralNetrowkClassifier:\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose = True):\n",
    "        self.lr = 0.01\n",
    "        self.loss_list = []\n",
    "        self.epoch_loss_list = []\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = 20\n",
    "        self.n_features = 784\n",
    "        self.n_nodes1 = 400\n",
    "        self.n_nodes2 = 200\n",
    "        self.n_output = 10\n",
    "        self.sigma = 0.01\n",
    "        self.W1 = np.random.randn(self.n_features,\n",
    "                                               self.n_nodes1) / np.sqrt(self.n_features)\n",
    "        self.W1_list = []\n",
    "        #print('初期値W1:', self.W1)\n",
    "        self.b1 = np.random.randn(self.n_nodes1,) / np.sqrt(self.n_features)\n",
    "        self.W2 = self.sigma * np.random.randn(self.n_nodes1,\n",
    "                                               self.n_nodes2) / np.sqrt(self.n_nodes1)\n",
    "        self.W2_list = []\n",
    "        #print('初期値W2:', self.W2)\n",
    "        self.b2 = np.random.randn(self.n_nodes2,) / np.sqrt(self.n_nodes1)\n",
    "        self.W3 = self.sigma * np.random.randn(self.n_nodes2, \n",
    "                                               self.n_output) / np.sqrt(self.n_nodes2)\n",
    "        self.W3_list = []\n",
    "        #print('初期値W3:', self.W3)\n",
    "        self.b3 = np.random.randn(self.n_output,) / np.sqrt(self.n_nodes2)\n",
    "        \n",
    "        self.b1_list = []\n",
    "        \n",
    "        \n",
    "        self.a1 = None\n",
    "        self.z1 = None\n",
    "        self.a2 = None\n",
    "        self.z2 = None\n",
    "        self.a3 = None\n",
    "        self.z3 = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dW1 = np.zeros_like(self.W1)\n",
    "        self.dW1_list = []\n",
    "        self.epoch_dW1_list = []\n",
    "        self.db1 = np.zeros_like(self.b1)\n",
    "        self.dW2 = np.zeros_like(self.W2)\n",
    "        self.db2 = np.zeros_like(self.b2)\n",
    "        self.dW3 = np.zeros_like(self.W3)\n",
    "        self.db3 = np.zeros_like(self.b3)\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        print('====fit処理を実行します。====')\n",
    "        \n",
    "        self.epochs = 20\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            print('************' + str(i+1) + '回目エポック**************')\n",
    "            \n",
    "        \n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "            count = 1\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                if self.verbose:\n",
    "#                     print(mini_y_train.shape)\n",
    "\n",
    "#                     print('mini_X_trainのタイプ：', type(mini_X_train))\n",
    "#                     print('mini_y_trainのタイプ：', type(mini_y_train))\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "#                     print('-------' + str(count) + '回目の処理----------')\n",
    "\n",
    "\n",
    "#                     print('--------self.lossを実行します---------')\n",
    "                    z3,loss = self.loss(mini_X_train, mini_y_train)\n",
    "#                     print('-------self.lossの実行を終了します。中身;loss: {}, z3: {} --------------'.format(loss, z3[:5,:]))\n",
    "#                     print('lossの形状：',loss.shape)\n",
    "                    self.loss_list.append(loss)\n",
    "#                     print('--------backward処理を実行します---------')\n",
    "                    self.backward(mini_X_train, mini_y_train, self.z3)\n",
    "#                     print('--------backward処理の実行を終了します---------')\n",
    "\n",
    "#                     print('------パラメータの更新処理を実行します------')\n",
    "\n",
    "                    self.W1 -= self.lr * self.dW1\n",
    "                    self.b1 -= self.lr * self.db1\n",
    "                    self.b1_list.append(self.b1)\n",
    "                    self.W2 -= self.lr * self.dW2\n",
    "                    self.b2 -= self.lr * self.db2\n",
    "                    self.W3 -= self.lr * self.dW3\n",
    "                    self.b3 -= self.lr * self.db3\n",
    "\n",
    "#                     print('------パラメータの更新処理の実行を終了します------')\n",
    "                    count += 1\n",
    "                    \n",
    "             \n",
    "                else:\n",
    "\n",
    "                    z3,y_pred = self.loss(mini_X_train, mini_y_train)\n",
    "#                     print('lossの形状：',loss.shape)\n",
    "                    self.loss_list.append(loss)\n",
    "                    self.backward(mini_X_train, mini_y_train, z3)\n",
    "\n",
    "\n",
    "\n",
    "                    self.W1 -= self.lr * self.dW1\n",
    "                    self.b1 -= self.lr * self.db1\n",
    "                    self.W2 -= self.lr * self.dW2\n",
    "                    self.b2 -= self.lr * self.db2\n",
    "                    self.W3 -= self.lr * self.dW3\n",
    "                    self.b3 -= self.lr * self.db3\n",
    "                    count += 1\n",
    "                    \n",
    "            self.epoch_loss_list.append(self.loss_list[-1])\n",
    "            #self.epoch_dW1_list.append(self.dW1_list[-1][0,0])\n",
    "            self.W1_list.append(self.W1)\n",
    "            self.W2_list.append(self.W2)\n",
    "            self.W3_list.append(self.W3)\n",
    "            print('loss: {}'.format(self.loss_list[-1]))\n",
    "            self.loss_list = []\n",
    "            self.dW1_list = []\n",
    "\n",
    "#             \"\"\"\n",
    "#             ニューラルネットワーク分類器を学習する。\n",
    "\n",
    "#             Parameters\n",
    "#             ----------\n",
    "#             X : 次の形のndarray, shape (n_samples, n_features)\n",
    "#                 訓練用データの特徴量\n",
    "#             y : 次の形のndarray, shape (n_samples, )\n",
    "#                 訓練用データの正解値\n",
    "#             X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "#                 検証用データの特徴量\n",
    "#             y_val : 次の形のndarray, shape (n_samples, )\n",
    "#                 検証用データの正解値\n",
    "#             \"\"\"\n",
    "        \n",
    "    def Sigmoid(self, y):\n",
    "        \n",
    "        #print('sigmoidの通す値：', y[:2,:3])\n",
    "        z = 1 / (1 + np.exp(-y))\n",
    "        #print('zの値：', z[:5,:5])\n",
    "        return z\n",
    "    \n",
    "#     def Softmax(self, a):\n",
    "        \n",
    "#         C = np.max(a, axis=1).reshape(-1,1)\n",
    "#         a_exp = np.exp(a - C)\n",
    "#         #print('a_expの形状：', a_exp.shape)\n",
    "#         a_sum = np.sum(a_exp,axis=1).reshape(-1,1)\n",
    "#         #print('a_sumの形状：', a_sum.shape)\n",
    "#         z = a_exp / a_sum\n",
    "#         print('z3の値：', z[:2,:5])\n",
    "#         print('zsumの値：', np.sum(z[0,:]))\n",
    "#         return z\n",
    "\n",
    "    def Softmax(self, x):\n",
    "        if x.ndim == 2:\n",
    "            #print('xの値：', x[:2,:5])\n",
    "            x = x.T\n",
    "            x = x - np.max(x, axis=0)\n",
    "            y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "            z3 = y.T\n",
    "            #print('z3の値：', z3[:2,:5])\n",
    "            return z3\n",
    "        x = x - np.max(x)   # オーバーフロー対策\n",
    "        z3 = np.exp(x) / np.sum(np.exp(x))\n",
    "        #print('z3の値：', z3[:2,:5])\n",
    "        return z3\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.a1 = np.dot(X, self.W1) + self.b1\n",
    "        #print('a1の中身：', self.a1[:3,:3])\n",
    "        self.z1 = self.Sigmoid(self.a1)\n",
    "        #print('z1の中身：', self.z1[:3,:3])\n",
    "        self.a2 = np.dot(self.z1, self.W2) + self.b2\n",
    "        #print('a2の中身：', self.a2[:3,:3])\n",
    "        self.z2 = self.Sigmoid(self.a2)\n",
    "        #print('z2の中身：', self.z2[:3,:3])\n",
    "        self.a3 = np.dot(self.z2, self.W3) + self.b3\n",
    "        #print('a3の中身：', self.a3[:3,:3])\n",
    "        self.z3 = self.Softmax(self.a3)\n",
    "        #print('z3の中身：', self.z3[:3,:3])\n",
    "        return self.z3\n",
    "    \n",
    "    \n",
    "    def backward(self, X, y, y_pred):\n",
    "        \n",
    "#         print('y_predのタイプ：',type(y_pred))\n",
    "#         print('yのタイプ：',type(y))\n",
    "#         print('Xのタイプ：', type(X))\n",
    "        #print('yの中身:', y[:3,:])\n",
    "        #print('y_predの中身：', y_pred[:3,:])\n",
    "        da3 = self.backprop_from_SoftmaxWithLoss(y, y_pred)\n",
    "        #print('da3の中身：', da3[:3,:])\n",
    "        #print('da3のタイプ：', type(da3))\n",
    "        dz2,self.dW3,self.db3 = self.backprop_Mutmul_layer(self.z2, self.W3, self.b3, da3)\n",
    "        #print('dz2の中身：', dz2[:3,:])\n",
    "#         print('dz2のタイプ：', type(dz2))\n",
    "        da2 = self.backprop_Sigmoid_layer(self.z2, dz2)#第一→全結合の値。\n",
    "        #print('da2の中身：', da2[:3,:])\n",
    "#         print('da2のタイプ：', type(da2))\n",
    "        dz1,self.dW2,self.db2 = self.backprop_Mutmul_layer(self.z1, self.W2, self.b2, da2)\n",
    "        #print('dz1の中身：', dz1[:3,:])\n",
    "#         print('dz1のタイプ：', type(dz1))\n",
    "        da1 = self.backprop_Sigmoid_layer(self.z1, dz1)\n",
    "        #print('da1の中身：', da1[:3,:])\n",
    "#         print('da1のタイプ：', type(da1))\n",
    "        dX, self.dW1, self.db1= self.backprop_Mutmul_layer(X, self.W1, self.b1, da1)\n",
    "        #print('dXの中身：', dX[:3,:])\n",
    "#         print('dXのタイプ：', type(dX))\n",
    "        \n",
    "        \n",
    "    \n",
    "    #平均交差エントロピーを返す\n",
    "    def crossentropy(self,y,t,delta = 1e-7):\n",
    "#         print('クロスエントロピーの形状：', (np.sum(t * np.log(y)) / t.shape[0]).shape)\n",
    "        return - np.sum(t * np.log(y + delta)) / t.shape[0]\n",
    "    \n",
    "        \n",
    "    \n",
    "    def loss(self, X, t):\n",
    "        y_pred = self.forward(X)\n",
    "#         print('y_predshape: ', y_pred.shape)\n",
    "        loss = self.crossentropy(y_pred, t)\n",
    "#         print('lossの形状：{}'.format(loss.shape))\n",
    "        return y_pred, loss\n",
    "    \n",
    "    \n",
    "    def backprop_from_SoftmaxWithLoss(self, t, y, dout=1):\n",
    "#         print('-----SoftmaxWithlossを実行します----')\n",
    "#         print(type(y), 'yのタイプ')\n",
    "        \n",
    "        #print('tの中身：', t[:3,:])\n",
    "#         print(type(t), 'tのタイプ')\n",
    "        return y - t\n",
    "    \n",
    "    def backprop_Mutmul_layer(self, X,W, b, dout):\n",
    "        #print('doutの形状', dout.shape)\n",
    "        #print('Xの形状',X.shape)\n",
    "        #print('Xの中身：', X[:2,:2])\n",
    "        #print('Wの中身：', W[:2,:2])\n",
    "        #print('bの中身：', b[:2])\n",
    "        #print('doutの中身；', dout[:2,:2])\n",
    "        dX = np.dot(dout, W.T)\n",
    "        dW = np.dot(X.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        #print('dWの形状', dW.shape)\n",
    "        #print('dXの形状', dX.shape)\n",
    "        #print('dbの形状', db.shape)\n",
    "        return dX, dW,db\n",
    "        \n",
    "        \n",
    "    def backprop_Sigmoid_layer(self, a, dout):\n",
    "        #print('勾配消失が起きているか確認：',a * (1 - a))\n",
    "        return a * (1 - a) * dout\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        z3 = self.forward(X)\n",
    "        \n",
    "        y_pred = np.argmax(z3,axis=1)\n",
    "        return y_pred\n",
    "        \n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "       \n",
    "       サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "\n",
    "    def accuracy(self,X,y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_argmax = np.argmax(y,axis=1)\n",
    "        print('y_pred:', y_pred)\n",
    "        print('y:', y_argmax)\n",
    "        print(np.sum(y_pred == y_argmax))\n",
    "        return np.sum(y_pred == y_argmax) * 100 /len(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "====fit処理を実行します。====\n",
      "************1回目エポック**************\n",
      "loss: 1.37590929217452\n",
      "************2回目エポック**************\n",
      "loss: 0.2209877143861152\n",
      "************3回目エポック**************\n",
      "loss: 0.14268468693901792\n",
      "************4回目エポック**************\n",
      "loss: 0.1018407824985729\n",
      "************5回目エポック**************\n",
      "loss: 0.08489673213024031\n",
      "************6回目エポック**************\n",
      "loss: 0.07606723062823376\n",
      "************7回目エポック**************\n",
      "loss: 0.06964019336382762\n",
      "************8回目エポック**************\n",
      "loss: 0.06501256274324213\n",
      "************9回目エポック**************\n",
      "loss: 0.06040045120678729\n",
      "************10回目エポック**************\n",
      "loss: 0.05447328215328416\n",
      "************11回目エポック**************\n",
      "loss: 0.04726232971070158\n",
      "************12回目エポック**************\n",
      "loss: 0.0408223713453368\n",
      "************13回目エポック**************\n",
      "loss: 0.03638261462400398\n",
      "************14回目エポック**************\n",
      "loss: 0.03301747068730719\n",
      "************15回目エポック**************\n",
      "loss: 0.029717452164063546\n",
      "************16回目エポック**************\n",
      "loss: 0.02592633777830325\n",
      "************17回目エポック**************\n",
      "loss: 0.02169224926202494\n",
      "************18回目エポック**************\n",
      "loss: 0.017585906004646885\n",
      "************19回目エポック**************\n",
      "loss: 0.014147409219833434\n",
      "************20回目エポック**************\n",
      "loss: 0.01151818534902525\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "network = ScratchSimpleNeuralNetrowkClassifier()\n",
    "print(y_train_one_hot.shape)\n",
    "network.fit(X_train, y_train_one_hot)\n",
    "print(len(network.loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "h = network.W3_list[0].shape[0]\n",
    "w = network.W3_list[0].shape[1]\n",
    "short = np.zeros((len(network.W3_list), 3, 3))\n",
    "for idx, w3_list in enumerate(network.W3_list):\n",
    "    short[idx,:,:] = w3_list[:3,:3]\n",
    "short = short.reshape(len(network.W3_list),-1)\n",
    "for i in range(short.shape[1]):\n",
    "    plt.plot(np.arange(1,short.shape[0] + 1), short[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.20600674e-04,  1.09573658e-03,  1.07553145e-03, ...,\n",
      "        -4.35060010e-04, -2.06489880e-05,  4.51479549e-04],\n",
      "       [ 4.21262705e-04,  6.27789852e-04, -1.45985149e-04, ...,\n",
      "        -1.16805949e-03, -2.59014644e-04,  7.83695155e-04],\n",
      "       [ 5.30771852e-04, -3.27636603e-04,  2.28240692e-04, ...,\n",
      "        -7.79966223e-04,  5.94829266e-04,  6.20709790e-04],\n",
      "       ...,\n",
      "       [ 7.94674080e-04, -7.95359395e-05, -1.51286533e-03, ...,\n",
      "        -1.70883341e-03, -3.92546105e-05,  9.36192737e-04],\n",
      "       [ 8.80676485e-04, -5.49888815e-04,  1.38455673e-04, ...,\n",
      "        -2.73868381e-04,  9.00597032e-04, -3.48754627e-04],\n",
      "       [ 5.70051966e-04, -2.95821719e-04,  6.33894037e-04, ...,\n",
      "        -4.35576257e-04,  5.15412159e-06,  1.12806526e-03]]), array([[ 1.20600674e-04,  1.09573658e-03,  1.07553145e-03, ...,\n",
      "        -4.35060010e-04, -2.06489880e-05,  4.51479549e-04],\n",
      "       [ 4.21262705e-04,  6.27789852e-04, -1.45985149e-04, ...,\n",
      "        -1.16805949e-03, -2.59014644e-04,  7.83695155e-04],\n",
      "       [ 5.30771852e-04, -3.27636603e-04,  2.28240692e-04, ...,\n",
      "        -7.79966223e-04,  5.94829266e-04,  6.20709790e-04],\n",
      "       ...,\n",
      "       [ 7.94674080e-04, -7.95359395e-05, -1.51286533e-03, ...,\n",
      "        -1.70883341e-03, -3.92546105e-05,  9.36192737e-04],\n",
      "       [ 8.80676485e-04, -5.49888815e-04,  1.38455673e-04, ...,\n",
      "        -2.73868381e-04,  9.00597032e-04, -3.48754627e-04],\n",
      "       [ 5.70051966e-04, -2.95821719e-04,  6.33894037e-04, ...,\n",
      "        -4.35576257e-04,  5.15412159e-06,  1.12806526e-03]]), array([[ 1.20600674e-04,  1.09573658e-03,  1.07553145e-03, ...,\n",
      "        -4.35060010e-04, -2.06489880e-05,  4.51479549e-04],\n",
      "       [ 4.21262705e-04,  6.27789852e-04, -1.45985149e-04, ...,\n",
      "        -1.16805949e-03, -2.59014644e-04,  7.83695155e-04],\n",
      "       [ 5.30771852e-04, -3.27636603e-04,  2.28240692e-04, ...,\n",
      "        -7.79966223e-04,  5.94829266e-04,  6.20709790e-04],\n",
      "       ...,\n",
      "       [ 7.94674080e-04, -7.95359395e-05, -1.51286533e-03, ...,\n",
      "        -1.70883341e-03, -3.92546105e-05,  9.36192737e-04],\n",
      "       [ 8.80676485e-04, -5.49888815e-04,  1.38455673e-04, ...,\n",
      "        -2.73868381e-04,  9.00597032e-04, -3.48754627e-04],\n",
      "       [ 5.70051966e-04, -2.95821719e-04,  6.33894037e-04, ...,\n",
      "        -4.35576257e-04,  5.15412159e-06,  1.12806526e-03]]), array([[ 1.20600674e-04,  1.09573658e-03,  1.07553145e-03, ...,\n",
      "        -4.35060010e-04, -2.06489880e-05,  4.51479549e-04],\n",
      "       [ 4.21262705e-04,  6.27789852e-04, -1.45985149e-04, ...,\n",
      "        -1.16805949e-03, -2.59014644e-04,  7.83695155e-04],\n",
      "       [ 5.30771852e-04, -3.27636603e-04,  2.28240692e-04, ...,\n",
      "        -7.79966223e-04,  5.94829266e-04,  6.20709790e-04],\n",
      "       ...,\n",
      "       [ 7.94674080e-04, -7.95359395e-05, -1.51286533e-03, ...,\n",
      "        -1.70883341e-03, -3.92546105e-05,  9.36192737e-04],\n",
      "       [ 8.80676485e-04, -5.49888815e-04,  1.38455673e-04, ...,\n",
      "        -2.73868381e-04,  9.00597032e-04, -3.48754627e-04],\n",
      "       [ 5.70051966e-04, -2.95821719e-04,  6.33894037e-04, ...,\n",
      "        -4.35576257e-04,  5.15412159e-06,  1.12806526e-03]]), array([[ 1.20600674e-04,  1.09573658e-03,  1.07553145e-03, ...,\n",
      "        -4.35060010e-04, -2.06489880e-05,  4.51479549e-04],\n",
      "       [ 4.21262705e-04,  6.27789852e-04, -1.45985149e-04, ...,\n",
      "        -1.16805949e-03, -2.59014644e-04,  7.83695155e-04],\n",
      "       [ 5.30771852e-04, -3.27636603e-04,  2.28240692e-04, ...,\n",
      "        -7.79966223e-04,  5.94829266e-04,  6.20709790e-04],\n",
      "       ...,\n",
      "       [ 7.94674080e-04, -7.95359395e-05, -1.51286533e-03, ...,\n",
      "        -1.70883341e-03, -3.92546105e-05,  9.36192737e-04],\n",
      "       [ 8.80676485e-04, -5.49888815e-04,  1.38455673e-04, ...,\n",
      "        -2.73868381e-04,  9.00597032e-04, -3.48754627e-04],\n",
      "       [ 5.70051966e-04, -2.95821719e-04,  6.33894037e-04, ...,\n",
      "        -4.35576257e-04,  5.15412159e-06,  1.12806526e-03]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(network.W2_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題7 学習曲線のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13d84ac88>]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(np.arange(1,len(network.epoch_loss_list)+1), network.epoch_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13d1f3160>]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(1,len(network.epoch_dW1_list)+1), network.epoch_dW1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]), array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.epoch_dW1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred: [1 1 6 ... 7 7 4]\n",
      "y: [1 1 6 ... 7 7 4]\n",
      "47739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99.45625"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.accuracy(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.epoch_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = []\n",
    "for i in range(5):\n",
    "    test_list.append(network.b1_list[i][0])\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s 254us/step - loss: 2.3304 - accuracy: 0.0985 - val_loss: 2.2974 - val_accuracy: 0.1037\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 2.2940 - accuracy: 0.1133 - val_loss: 2.2891 - val_accuracy: 0.1151\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 2.2862 - accuracy: 0.1187 - val_loss: 2.2814 - val_accuracy: 0.1361\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 2s 240us/step - loss: 2.2783 - accuracy: 0.1445 - val_loss: 2.2738 - val_accuracy: 0.1546\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 2.2708 - accuracy: 0.1624 - val_loss: 2.2657 - val_accuracy: 0.1266\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 3s 264us/step - loss: 2.2627 - accuracy: 0.1845 - val_loss: 2.2581 - val_accuracy: 0.1666\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 3s 250us/step - loss: 2.2548 - accuracy: 0.2050 - val_loss: 2.2501 - val_accuracy: 0.1529\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 2s 237us/step - loss: 2.2467 - accuracy: 0.2469 - val_loss: 2.2426 - val_accuracy: 0.1437\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 2s 242us/step - loss: 2.2389 - accuracy: 0.2306 - val_loss: 2.2338 - val_accuracy: 0.2676\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 3s 255us/step - loss: 2.2302 - accuracy: 0.2598 - val_loss: 2.2261 - val_accuracy: 0.4289\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import optimizers\n",
    "model = Sequential()\n",
    "model.add(Dense(400,input_dim=784, activation='sigmoid',\n",
    "               kernel_initializer='glorot_uniform'))\n",
    "\n",
    "model.add(Dense(200,activation='sigmoid',\n",
    "               kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(10,activation='softmax',\n",
    "               kernel_initializer='glorot_uniform'))\n",
    "\n",
    "sgd = keras.optimizers.SGD(lr=0.001)\n",
    "model.compile(optimizer=sgd,\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train[:10000,:], y_train[:10000], epochs=10,batch_size=20,\n",
    "                   verbose=1,validation_data = (X_train[:10000,:],y_train[:10000]))\n",
    "score = model.evaluate(X_train[:10000,:],y_train[:10000,:],verbose=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下の実行で初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.0\n",
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1,784)\n",
    "X_test = X_test.reshape(-1,784)\n",
    "\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.max())\n",
    "print(X_train.min())\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot,\n",
    "                                                 test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ここまで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0] / self.batch_size).astype(np.int)\n",
    "        #self.stopは作成するバッチサイズ数。１エポック分作成する\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    #指定したバッチ番号を取ってきてくれる\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0 : p1], self._y[p0 : p1]\n",
    "\n",
    "    \n",
    "    \n",
    "    #batchカウンターを初期化する\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    #batchを前から一つずつ取ってくる\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter * self.batch_size\n",
    "        p1 = self._counter * self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0 : p1], self._y[p0 : p1]\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " get_mini_batch = GetMiniBatch(X_train, y_train, batch_size=20)\n",
    "\n",
    "print(len(get_mini_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    #print(mini_X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題1 重みの初期値を決めるコードの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "n_features = 784\n",
    "n_nodes1 = 400\n",
    "n_nodes2 = 200\n",
    "n_output = 10\n",
    "sigma = 0.01\n",
    "W1 = sigma * np.random.randn(n_features, n_nodes1)\n",
    "b1 = np.random.randn(n_nodes1,)\n",
    "W2 = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "b2 = np.random.randn(n_nodes2,)\n",
    "W3 = sigma * np.random.randn(n_nodes2, n_output)\n",
    "b3 = np.random.randn(n_output,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題2 フォワードプロパゲーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sigmoid(y):\n",
    "    z = 1 / (1 + np.exp(-y))\n",
    "    return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(a,):\n",
    "    a_exp = np.exp(a)\n",
    "    a_sum = np.sum(a_exp)\n",
    "    z = a_exp / a_sum\n",
    "    return z\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedForward(X):\n",
    "    \n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    #print(a1[0],'a1')\n",
    "    z1 = Sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    #print(a2,'a2')\n",
    "    z2 = Sigmoid(a2)\n",
    "    #print(z2,'z2')\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    #print(a3, 'a3')\n",
    "    z3 = Softmax(a3)\n",
    "    #print(z3,'z3')\n",
    "    #print('z3のタイプ', type(z3))\n",
    "    return a1, z1, a2, z2, a3, z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mini, y_mini = get_mini_batch[0]\n",
    "\n",
    "a1, z1, a2, z2, a3,z3 = FeedForward(X_mini)\n",
    "print(z3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z3[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log(z3)[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題3 交差エントロピー誤差の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(X,t):\n",
    "        \n",
    "    a1, z1, a2, z2, a3, z3 = FeedForward(X)#y_pred(N,10), t(N,10)=(N,10)\n",
    "    print(np.sum(z3 < 0), 'マイナスのz3がないかチェック')\n",
    "\n",
    "    print(type(z3),'z3のタイプ')\n",
    "    print(type(t),'tのタイプ')\n",
    "    #print(z3,'z3')\n",
    "    print(np.log(z3 + 1e-20).shape,'logz3の形')\n",
    "    print(t.shape)\n",
    "    L = -np.sum(t * np.log(z3 + 1e-50))\n",
    "    return L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_mini_array = np.squeeze(np.asarray(y_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = crossentropy(X_mini,y_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(network.loss_list)+1, network.loss_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "network = ScratchSimpleNeuralNetrowkClassifier()\n",
    "for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    loss += network.loss(mini_X_train,mini_y_train)\n",
    "mean_loss = loss / len(get_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 問題4 バックプロパゲーションの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_from_SoftmaxWithLoss(t, y, dout=1):\n",
    "    return y - t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_Mutmul_layer(X,W, b, dout):\n",
    "    #print('doutの形状', dout.shape)\n",
    "    #print('Xの形状',X.shape)\n",
    "    dX = np.dot(dout, W.T)\n",
    "    dW = np.dot(X.T, dout)\n",
    "    db = np.sum(dout, axis=0)\n",
    "    #print('dWの形状', dW.shape)\n",
    "    #print('dXの形状', dX.shape)\n",
    "    #print('dbの形状', db.shape)\n",
    "    return dX, dW,db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop_Sigmoid_layer(dout):\n",
    "    return dout * (1 - dout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#あえてクラスを作成せずにやってみる\n",
    "#backward\n",
    "lr = 0.001\n",
    "loss_list = []\n",
    "batch_size = 20\n",
    "n_features = 784\n",
    "n_nodes1 = 400\n",
    "n_nodes2 = 200\n",
    "n_output = 10\n",
    "sigma = 0.01\n",
    "W1 = sigma * np.random.randn(n_features, n_nodes1)\n",
    "b1 = np.random.randn(n_nodes1,)\n",
    "W2 = sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "b2 = np.random.randn(n_nodes2,)\n",
    "W3 = sigma * np.random.randn(n_nodes2, n_output)\n",
    "b3 = np.random.randn(n_output,)\n",
    "\n",
    "\n",
    "for mini_X_train, mini_y_train in get_mini_batch:\n",
    "    #print(mini_X_train[0])\n",
    "    print(W1.shape)\n",
    "    print(type(mini_X_train),'mini_X_trainのタイプ')\n",
    "    a1,z1, a2, z2, a3, z3 = FeedForward(mini_X_train)\n",
    "    \n",
    "    \n",
    "    #print(z3,'z3の値')\n",
    "    da3 = backprop_from_SoftmaxWithLoss(mini_y_train, z3)\n",
    "    dz2,dW3,db3 = backprop_Mutmul_layer(z2, W3, b3, da3)\n",
    "    da2 = backprop_Sigmoid_layer(dz2)\n",
    "    dz1,dW2,db2 = backprop_Mutmul_layer(z1, W2, b2, da2)\n",
    "    da1 = backprop_Sigmoid_layer(dz1)\n",
    "    dX, dW1, db1= backprop_Mutmul_layer(mini_X_train, W1, b1, da1)\n",
    "\n",
    "        #勾配の更新\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W3 -= lr * dW3\n",
    "    b3 -= lr * db3\n",
    "    \n",
    "    #lossを求める\n",
    "    loss = crossentropy(mini_X_train,mini_y_train)\n",
    "    #print(loss)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(np.arange(1,len(loss_list) + 1), loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss_list[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([[8,8,9],\n",
    "                       [9,1,2]])\n",
    "print(np.argmax(test_array,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = np.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#coding: utf-8\n",
    "import numpy as np\n",
    "\n",
    "#from common.util import im2col, col2im\n",
    "\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W =W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 重み・バイアスパラメータの微分\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # テンソル対応\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None # softmaxの出力\n",
    "        self.t = None # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None # Conv層の場合は4次元、全結合層の場合は2次元  \n",
    "\n",
    "        # テスト時に使用する平均と分散\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        # backward時に使用する中間データ\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, train_flg)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if train_flg:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        # 中間データ（backward時に使用）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 重み・バイアスパラメータの勾配\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "class TestLayerNet:\n",
    "    \"\"\"全結合による多層ニューラルネットワーク\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 入力サイズ（MNISTの場合は784）\n",
    "    hidden_size_list : 隠れ層のニューロンの数のリスト（e.g. [100, 100, 100]）\n",
    "    output_size : 出力サイズ（MNISTの場合は10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "        'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "        'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "    weight_decay_lambda : Weight Decay（L2ノルム）の強さ\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {}\n",
    "\n",
    "        # 重みの初期化\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # レイヤの生成\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "            self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"重みの初期値設定\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 重みの標準偏差を指定（e.g. 0.01）\n",
    "            'relu'または'he'を指定した場合は「Heの初期値」を設定\n",
    "            'sigmoid'または'xavier'を指定した場合は「Xavierの初期値」を設定\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLUを使う場合に推奨される初期値\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoidを使う場合に推奨される初期値\n",
    "\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"損失関数を求める\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        損失関数の値\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "#     def numerical_gradient(self, x, t):\n",
    "#         \"\"\"勾配を求める（数値微分）\n",
    "\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         x : 入力データ\n",
    "#         t : 教師ラベル\n",
    "\n",
    "#         Returns\n",
    "#         -------\n",
    "#         各層の勾配を持ったディクショナリ変数\n",
    "#             grads['W1']、grads['W2']、...は各層の重み\n",
    "#             grads['b1']、grads['b2']、...は各層のバイアス\n",
    "#         \"\"\"\n",
    "#         loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "#         grads = {}\n",
    "#         for idx in range(1, self.hidden_layer_num+2):\n",
    "#             grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "#             grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "\n",
    "#         return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（誤差逆伝搬法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "\n",
    "    \"\"\"確率的勾配降下法（Stochastic Gradient Descent）\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シグモイドの微分値を調べれば勾配が消失しているか確認できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
